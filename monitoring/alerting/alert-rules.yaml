apiVersion: 1
groups:
  - orgId: 1
    name: SAS Viya Alerts
    folder: CAS Alerts
    interval: 5m
    rules:
      - uid: fc41d560-9a18-4168-8a6a-615e60dc70de
        title: cas-restart
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              disableTextWrap: false
              editorMode: code
              expr: cas_grid_uptime_seconds_total
              fullMetaSearch: false
              includeNullMetadata: true
              instant: true
              intervalMs: 1000
              legendFormat: __auto
              maxDataPoints: 43200
              range: false
              refId: A
              useBackend: false
          - refId: B
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params: []
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - B
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              intervalMs: 1000
              maxDataPoints: 43200
              reducer: last
              refId: B
              type: reduce
          - refId: C
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 900
                    type: lt
                  operator:
                    type: and
                  query:
                    params:
                      - C
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: B
              intervalMs: 1000
              maxDataPoints: 43200
              refId: C
              type: threshold
        noDataState: NoData
        execErrState: Error
        for: 5m
        annotations:
          description:
            Check to see that the CAS pod existed for a short time.  This implies
            that CAS pod has restarted for whatever the reason.  Will need to further
            investigate the cause.
          summary:
            The current CAS (sas-cas-server-default-controller) pod < 15 minutes
            in existence.  Mostly likely it is due to restart of the CAS pod.
        labels: {}
        isPaused: false
      - uid: ca744a08-e4e9-49b7-85a1-79e9fe05d4c1
        title: cas-memory
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              editorMode: code
              exemplar: false
              expr: (cas_node_mem_size_bytes{type="physical"} - cas_node_mem_free_bytes{type="physical"})/1073741824
              instant: true
              interval: ""
              intervalMs: 1000
              legendFormat: __auto
              maxDataPoints: 43200
              range: false
              refId: A
          - refId: B
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params: []
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - B
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              intervalMs: 1000
              maxDataPoints: 43200
              reducer: last
              refId: B
              type: reduce
          - refId: C
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 300
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - C
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: B
              intervalMs: 1000
              maxDataPoints: 43200
              refId: C
              type: threshold
        noDataState: NoData
        execErrState: Error
        for: 5m
        annotations:
          description:
            Checks the CAS memory usage.  If it is > 300GB, it will alert.  Currently,
            max. memory is 512GB.  The expectation is that this alert will be an early
            warning sign to investigate large memory usage as typical usage is less than
            the threshold.  Want to prevent OOMkill of CAS.
          summary:
            CAS memory > 300GB.  This can be due to a program or pipeline taking
            all the available memory.
        labels: {}
        isPaused: false
      - uid: cas_restart_alert
        title: CAS Restart Alert
        condition: A
        for: 5m
        labels:
          severity: warning
        annotations:
          description:
            Check to see that the CAS pod existed for a short time. This implies
            that CAS pod has restarted for whatever the reason. Will need to further investigate
            the cause.
          summary:
            The current CAS (sas-cas-server-default-controller) pod < 15 minutes
            in existence. Mostly likely it is due to restart of the CAS pod.
        data:
          - refId: A
            datasourceUid: prometheus
            relativeTimeRange:
              from: 300
              to: 0
            model:
              expr: cas_grid_uptime_seconds_total
              instant: true
      - uid: cas_memory_usage
        title: CAS Memory Usage
        condition: A
        for: 5m
        labels:
          severity: warning
        annotations:
          description: Checks the CAS memory usage. If it is > 300GB, it will alert.
          summary:
            CAS memory > 300GB. This can be due to a program or pipeline taking
            all the available memory.
        data:
          - refId: A
            datasourceUid: prometheus
            relativeTimeRange:
              from: 300
              to: 0
            model:
              expr:
                (cas_node_mem_size_bytes{type="physical"} - cas_node_mem_free_bytes{type="physical"})
                / 1073741824 > 300
              instant: true
  - orgId: 1
    name: SAS Viya Alerts
    folder: Viya Platform Alerts
    interval: 5m
    rules:
      - uid: e45e6d74-e396-40ce-a061-2a294295e61b
        title: viya-readiness
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              disableTextWrap: false
              editorMode: builder
              expr: kube_pod_container_status_ready{container="sas-readiness"}
              fullMetaSearch: false
              includeNullMetadata: true
              instant: true
              intervalMs: 1000
              legendFormat: __auto
              maxDataPoints: 43200
              range: false
              refId: A
              useBackend: false
          - refId: B
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params: []
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - B
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              intervalMs: 1000
              maxDataPoints: 43200
              reducer: last
              refId: B
              type: reduce
          - refId: C
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 1
                    type: lt
                  operator:
                    type: and
                  query:
                    params:
                      - C
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: B
              intervalMs: 1000
              maxDataPoints: 43200
              refId: C
              type: threshold
        noDataState: NoData
        execErrState: Error
        for: 5m
        annotations:
          description:
            Checks for the Ready state of sas-readiness pod.  Will need to
            check the status of the Viya pods since sas-readiness pod reflects the health
            of the Viya services.
          summary:
            sas-readiness pod is not in Ready state.  This means that one or more
            of the Viya services are not in a good state.
        labels: {}
        isPaused: false
      - uid: efb36686-4e44-4de8-80c4-7dde9130da90
        title: rabbitmq-readymessages
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              disableTextWrap: false
              editorMode: builder
              expr: rabbitmq_queue_messages_ready
              fullMetaSearch: false
              includeNullMetadata: true
              instant: true
              intervalMs: 1000
              legendFormat: __auto
              maxDataPoints: 43200
              range: false
              refId: A
              useBackend: false
          - refId: B
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params: []
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - B
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              intervalMs: 1000
              maxDataPoints: 43200
              reducer: last
              refId: B
              type: reduce
          - refId: C
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 10000
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - C
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: B
              intervalMs: 1000
              maxDataPoints: 43200
              refId: C
              type: threshold
        noDataState: NoData
        execErrState: Error
        for: 5m
        annotations:
          description:
            Checks for accumulation of Rabbitmq ready messages > 10,000.  It
            could impact Model Studio pipelines.  Follow the steps in the runbook url
            to help troubleshoot.  The runbook covers potential orphan queues and/or bottlenecking
            of queues due to catalog service.
          runbook_url: https://sas-institute.atlassian.net/wiki/spaces/ITSSA/pages/1032552466/Troubleshooting+a+model+studio+project+where+the+Data+node+was+stuck+on+Running
          summary:
            Rabbitmq ready messages > 10,000.  This means there is a large backlog
            of messages due to high activity (which can be temporary) or something has
            gone wrong.
        labels: {}
        isPaused: false
      - uid: ed69b8e4-ce60-44a0-8f51-83743df0e448
        title: compute-age
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              editorMode: code
              expr: (time() - kube_pod_created{pod=~"sas-compute-server-.*"})/60/60/24
              instant: true
              intervalMs: 1000
              legendFormat: __auto
              maxDataPoints: 43200
              range: false
              refId: A
          - refId: B
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params: []
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - B
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              intervalMs: 1000
              maxDataPoints: 43200
              reducer: last
              refId: B
              type: reduce
          - refId: C
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 1
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - C
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: B
              intervalMs: 1000
              maxDataPoints: 43200
              refId: C
              type: threshold
        noDataState: OK
        execErrState: Error
        for: 5m
        annotations:
          description:
            "It looks for compute pods > 1 day.  Most likely, it is orphaned
            compute pod that is lingering.  Consider killing it.

            There is an airflow job that sweeps the VFL fleet regularly to look for these
            compute pods as well for deletion."
          summary:
            SAS compute-server pods > 1 day old. Compute pods in VFL do not need
            to be running longer than 1 day since there are no long running jobs.
        labels: {}
        isPaused: true
      - uid: e7ecb843-f1bd-48b7-8c8c-58571d1642ad
        title: viya-pod-restarts
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              disableTextWrap: false
              editorMode: builder
              expr: kube_pod_container_status_restarts_total{namespace="viya"}
              fullMetaSearch: false
              includeNullMetadata: true
              instant: true
              intervalMs: 1000
              legendFormat: __auto
              maxDataPoints: 43200
              range: false
              refId: A
              useBackend: false
          - refId: B
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params: []
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - B
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              intervalMs: 1000
              maxDataPoints: 43200
              reducer: last
              refId: B
              type: reduce
          - refId: C
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 20
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - C
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: B
              intervalMs: 1000
              maxDataPoints: 43200
              refId: C
              type: threshold
        noDataState: NoData
        execErrState: Error
        for: 5m
        annotations:
          description:
            Checks the restart count of the pod(s).  Will need to check why
            the pod(s) have restarted so many times.  One possible cause is OOMkill.  This
            means we will need to increase the memory limit.
          summary:
            The number of pod restarts > 20.  The service pod(s) have restarted
            many times due to issues.
        labels: {}
        isPaused: false
      - uid: viya_readiness
        title: Viya Readiness
        condition: A
        for: 5m
        labels:
          severity: warning
        annotations:
          description:
            Checks for the Ready state of sas-readiness pod. Will need to check
            the status of the Viya pods since sas-readiness pod reflects the health of
            the Viya services.
          summary:
            sas-readiness pod is not in Ready state. This means that one or more
            of the Viya services are not in a good state.
        data:
          - refId: A
            datasourceUid: prometheus
            relativeTimeRange:
              from: 300
              to: 0
            model:
              expr: kube_pod_container_status_ready{container="sas-readiness"}
              instant: true
      - uid: rabbitmq_ready_msgs
        title: RabbitMQ Ready Messages
        condition: A
        for: 5m
        labels:
          severity: warning
        annotations:
          description:
            Checks for accumulation of Rabbitmq ready messages > 10,000. It
            could impact Model Studio pipelines.
          summary:
            Rabbitmq ready messages > 10,000. This means there is a large backlog
            of messages due to high activity or something has gone wrong.
        data:
          - refId: A
            datasourceUid: prometheus
            relativeTimeRange:
              from: 300
              to: 0
            model:
              expr: rabbitmq_queue_messages_ready > 10000
              instant: true
      - uid: compute_pod_age
        title: Compute Pod Age
        condition: A
        for: 5m
        labels:
          severity: warning
        annotations:
          description: Looks for compute pods > 1 day.
          summary: SAS compute-server pods > 1 day old.
        data:
          - refId: A
            datasourceUid: prometheus
            relativeTimeRange:
              from: 300
              to: 0
            model:
              expr:
                (time() - kube_pod_created{pod=~"sas-compute-server-.*"}) / 60 / 60
                / 24 > 1
              instant: true
      - uid: viya_pod_restarts
        title: Viya Pod Restarts
        condition: A
        for: 5m
        labels:
          severity: warning
        annotations:
          description: Checks if any Viya pods have restarted > 20 times.
          summary: The number of pod restarts > 20. Investigate for OOM or instability.
        data:
          - refId: A
            datasourceUid: prometheus
            relativeTimeRange:
              from: 300
              to: 0
            model:
              expr: kube_pod_container_status_restarts_total{namespace="viya"} > 20
              instant: true
  - orgId: 1
    name: SAS Viya Alerts
    folder: Other Alerts
    interval: 5m
    rules:
      - uid: d52b3c24-acf4-4b5e-ae52-31ff8f167330
        title: NFS-share
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              editorMode: code
              expr:
                ((kubelet_volume_stats_capacity_bytes{persistentvolumeclaim="cas-default-data"}
                - kubelet_volume_stats_available_bytes{persistentvolumeclaim="cas-default-data"})
                / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim="cas-default-data"})
                * 100
              instant: true
              intervalMs: 1000
              legendFormat: __auto
              maxDataPoints: 43200
              range: false
              refId: A
          - refId: B
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params: []
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - B
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              intervalMs: 1000
              maxDataPoints: 43200
              reducer: last
              refId: B
              type: reduce
          - refId: C
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 85
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - C
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: B
              intervalMs: 1000
              maxDataPoints: 43200
              refId: C
              type: threshold
        noDataState: NoData
        execErrState: Error
        for: 5m
        annotations:
          description:
            Checks if the NFS share attached to CAS is > 85% full.  Use command
            "du -h -d 1" to to find the location where large files are located in the
            NFS shares.  Most likely it will be one of the home directories due to runaway
            size of a casuser table or Viya backups.
          summary:
            NFS share > 85% full.  Typically, it is due to users filling their
            own home directory or backups.
        labels: {}
        isPaused: false
      - uid: abe80c6a-3add-477a-b228-f8283704570f
        title: crunchy-backrest-repo
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              editorMode: code
              expr:
                ((kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"sas-crunchy-platform-postgres-repo1"}
                - kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"sas-crunchy-platform-postgres-repo1"})
                / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"sas-crunchy-platform-postgres-repo1"})
                * 100
              instant: true
              intervalMs: 1000
              legendFormat: __auto
              maxDataPoints: 43200
              range: false
              refId: A
          - refId: B
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params: []
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - B
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              intervalMs: 1000
              maxDataPoints: 43200
              reducer: last
              refId: B
              type: reduce
          - refId: C
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 50
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - C
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: B
              intervalMs: 1000
              maxDataPoints: 43200
              refId: C
              type: threshold
        noDataState: NoData
        execErrState: Error
        for: 5m
        annotations:
          description:
            "Checks to see /pgbackrest/repo1 filesystem is more than 50% full.

            Go to the Runbook URL to follow the troubleshooting steps."
          runbook_url: https://sas-institute.atlassian.net/wiki/spaces/ITSSA/pages/1247543297/When+Crunchy+database+disk+space+is+full
          summary:
            /pgbackrest/repo1 storage > 50% full in the pgbackrest repo.  This
            typically happens when the archived WAL logs are increasing and not being
            expired and cleared.
        labels: {}
        isPaused: false
      - uid: nfs_share_usage
        title: NFS Share Usage
        condition: A
        for: 5m
        labels:
          severity: warning
        annotations:
          description: Checks if the NFS share attached to CAS is > 85% full.
          summary:
            NFS share > 85% full. Typically, it is due to users filling their home
            directory or backups.
        data:
          - refId: A
            datasourceUid: prometheus
            relativeTimeRange:
              from: 21600
              to: 0
            model:
              expr:
                "((kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=\"cas-default-data\"\
                }\n - kubelet_volume_stats_available_bytes{persistentvolumeclaim=\"cas-default-data\"\
                })\n / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=\"cas-default-data\"\
                }) * 100 > 85"
              instant: true
      - uid: pgbackrest_repo_usage
        title: Crunchy Backrest Repo Usage
        condition: A
        for: 5m
        labels:
          severity: warning
        annotations:
          description: Checks if /pgbackrest/repo1 is more than 50% full.
          summary:
            /pgbackrest/repo1 storage > 50% full. Possibly due to unexpired WAL
            logs.
        data:
          - refId: A
            datasourceUid: prometheus
            relativeTimeRange:
              from: 300
              to: 0
            model:
              expr:
                "((kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~\"sas-crunchy-platform-postgres-repo1\"\
                }\n - kubelet_volume_stats_available_bytes{persistentvolumeclaim=~\"sas-crunchy-platform-postgres-repo1\"\
                })\n / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~\"sas-crunchy-platform-postgres-repo1\"\
                }) * 100 > 50"
              instant: true
  - orgId: 1
    name: SAS Viya Alerts
    folder: Database Alerts
    interval: 5m
    rules:
      - uid: fc65fbaf-c196-4eb4-a130-f45cc46b775b
        title: catalog-dbconn
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              disableTextWrap: false
              editorMode: builder
              expr: sas_db_pool_connections{container="sas-catalog-services", state="inUse"}
              fullMetaSearch: false
              includeNullMetadata: true
              instant: true
              intervalMs: 1000
              legendFormat: __auto
              maxDataPoints: 43200
              range: false
              refId: A
              useBackend: false
          - refId: B
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params: []
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - B
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              intervalMs: 1000
              maxDataPoints: 43200
              reducer: last
              refId: B
              type: reduce
          - refId: C
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 21
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - C
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: B
              intervalMs: 1000
              maxDataPoints: 43200
              refId: C
              type: threshold
        noDataState: NoData
        execErrState: Error
        for: 5m
        annotations:
          description:
            "Checks the in-use catalog database connections > 21.  The default
            db connection pool is 22.   If it reaches the limit, the rabbitmq queues starts
            to fill up with ready messages causing issues with Model Studio pipelines.

            Click on the runbook URL on how to remediate the issue."
          runbook_url: https://sas-institute.atlassian.net/wiki/spaces/ITSSA/pages/1032552466/Troubleshooting+a+model+studio+project+where+the+Data+node+was+stuck+on+Running
          summary:
            The active catalog database connections > 21.  If it reaches the max.
            db connections, it will impact the rabbitmq queues.
        labels: {}
        isPaused: false
      - uid: fb411e28-b2e5-43d0-a413-e6dedbf154c4
        title: crunchy-pgdata
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: prometheus
            model:
              disableTextWrap: false
              editorMode: code
              expr:
                ((kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"sas-crunchy-platform-postgres-00-.*"}
                - kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"sas-crunchy-platform-postgres-00-.*"})
                / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"sas-crunchy-platform-postgres-00-.*"})
                * 100
              fullMetaSearch: false
              includeNullMetadata: true
              instant: true
              intervalMs: 1000
              legendFormat: __auto
              maxDataPoints: 43200
              range: false
              refId: A
              useBackend: false
          - refId: B
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params: []
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - B
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              intervalMs: 1000
              maxDataPoints: 43200
              reducer: last
              refId: B
              type: reduce
          - refId: C
            relativeTimeRange:
              from: 600
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 50
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - C
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: B
              intervalMs: 1000
              maxDataPoints: 43200
              refId: C
              type: threshold
        noDataState: NoData
        execErrState: Error
        for: 5m
        annotations:
          description: "Checks to see /pgdata filesystem is more than 50% full.

            Go to the Runbook URL to follow the troubleshooting steps."
          runbook_url: https://sas-institute.atlassian.net/wiki/spaces/ITSSA/pages/1247543297/When+Crunchy+database+disk+space+is+full
          summary:
            /pgdata storage > 50% full.  This typically happens when the WAL logs
            are increasing and not being cleared.
        labels: {}
        isPaused: false
      - uid: catalog_db_connections
        title: Catalog DB Connections
        condition: A
        for: 5m
        labels:
          severity: warning
        annotations:
          description: Checks the in-use catalog database connections > 21.
          summary: The active catalog database connections > 21. May impact RabbitMQ queues.
        data:
          - refId: A
            datasourceUid: prometheus
            relativeTimeRange:
              from: 300
              to: 0
            model:
              expr:
                sas_db_pool_connections{container="sas-catalog-services", state="inUse"}
                > 21
              instant: true
      - uid: pgdata_usage
        title: Crunchy PGData Usage
        condition: A
        for: 5m
        labels:
          severity: warning
        annotations:
          description: Checks if /pgdata is more than 50% full.
          summary: /pgdata storage > 50% full. Often due to WAL logs not being cleared.
        data:
          - refId: A
            datasourceUid: prometheus
            relativeTimeRange:
              from: 300
              to: 0
            model:
              expr:
                "((kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~\"sas-crunchy-platform-postgres-00-.*\"\
                }\n - kubelet_volume_stats_available_bytes{persistentvolumeclaim=~\"sas-crunchy-platform-postgres-00-.*\"\
                })\n / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~\"sas-crunchy-platform-postgres-00-.*\"\
                }) * 100 > 50"
              instant: true

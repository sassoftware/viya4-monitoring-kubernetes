# Override values for the Prometheus Operator helm chart
#
# Prometheus Operator Helm Chart
# https://https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack
#
# CRDs
# https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md
#
# Default Values
# https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml

commonLabels:
  sas.com/monitoring-base: kube-viya-monitoring

# ===================
# Prometheus Operator
# ===================
# https://github.com/coreos/prometheus-operator
prometheusOperator:
  image:
    registry: quay.io
    repository: prometheus-operator/prometheus-operator
    tag: v0.62.0
  logFormat: json
  logLevel: info
  createCustomResource: false
  resources:
    requests:
      cpu: "100m"
      memory: "50Mi"
    limits:
      cpu: "500m"
      memory: "2Gi"
  tlsProxy:
    resources:
      requests:
        cpu: "50m"
        memory: "25Mi"
  prometheusConfigReloader:
    image:
      registry: quay.io
      repository: prometheus-operator/prometheus-config-reloader
      tag: v0.62.0

# ======================
# kubelet ServiceMonitor
# ======================
kubelet:
  serviceMonitor:
    # Default to use the https-metrics endpoint
    # Depending on your environment, this may require configuration
    # changes to the kubelet.
    # See issue: https://github.com/coreos/prometheus-operator/issues/926
    cAdvisorMetricRelabelings: null
    https: true
    interval: 20s

# ======================
# kube-state-metrics
# ======================
kubeStateMetrics:
  serviceMonitor:
    interval: 20s

# https://github.com/kubernetes/kube-state-metrics
# https://github.com/helm/charts/tree/master/stable/kube-state-metrics
kube-state-metrics:
  image:
    tag: v2.7.0
  resources:
    requests:
      cpu: "25m"
      memory: "50Mi"
  # kube-state-metrics 2.0 requires explicitly defining which labels are
  # collected for each resource type 
  extraArgs:
  - --metric-labels-allowlist=nodes=[*],namespaces=[*],pods=[*],deployments=[*],statefulsets=[*],daemonsets=[*],jobs=[*]

# ==========
# Prometheus
# ==========
prometheus:
  serviceAccount:
    name: sas-ops-acct
  service:
    type: ClusterIP
    nodePort: null  
  prometheusSpec:
    image:
      tag: v2.40.7
    logLevel: info
    logFormat: json
    podAntiAffinity: soft
    # Don't restrict ServiceMonitor namespace selection by default
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    ruleSelectorNilUsesHelmValues: false
    retention: 7d
    retentionSize: 20GiB
    replicas: 1
    resources:
      # These values may very well need to be overridden to higher
      # values depending on the scale of the cluster
      requests:
        cpu: "1000m"
        memory: "2Gi"
    storageSpec:
      volumeClaimTemplate:
        spec:
          # storageClassName:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 25Gi    
          volumeMode: Filesystem
    alertingEndpoints:
      - name: v4m-alertmanager 
        port: http-web

# =======================
# Prometheus AlertManager
# =======================
alertmanager:
  service:
    type: ClusterIP
    nodePort: null
  alertmanagerSpec:
    image:
      tag: v0.25.0
    logFormat: json
    podAntiAffinity: soft
    retention: 240h
    resources:
      requests:
        cpu: "50m"
        memory: "50Mi"
    storage:
      volumeClaimTemplate:
        spec:
          # storageClassName:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 10Gi
          volumeMode: Filesystem

# Prometheus Node Exporter
# https://github.com/prometheus/node_exporter
# https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-node-exporter
prometheus-node-exporter:
  image:
    # registry: quay.io
    repository: quay.io/prometheus/node-exporter
    tag: v1.5.0
  service:
    # Override the default port of 9100 to avoid potential conflicts
    port: 9110
    targetPort: 9110
  resources:
    requests:
      cpu: "50m"
      memory: "50Mi"
    limits:
      cpu: "250m"
      memory: "100Mi"
  # Be very tolerant
  tolerations:
  - operator: "Exists"
  prometheus:
    monitor:
      relabelings:
      - sourceLabels: [__meta_kubernetes_pod_node_name]
        separator: ;
        regex: ^(.*)$
        targetLabel: nodename
        replacement: $1
        action: replace

# =======
# Grafana
# =======
# https://github.com/grafana/helm-charts/tree/main/charts/grafana
grafana:
  image:
    tag: "9.3.1"
  "grafana.ini":
    analytics:
      check_for_updates: false
    # dashboards:
    #   default_home_dashboard_path: 
    log:
      mode: console
    "log.console":
        format: json
    dashboards:
      default_home_dashboard_path: /tmp/dashboards/viya-welcome-dashboard.json
  service:
    type: NodePort
    nodePort: 31100
  plugins: []
  resources:
    requests:
      cpu: "250m"
      memory: "150Mi"
  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard
    datasources:
      enabled: true
      label: grafana_datasource
      resources:
    resources:
      requests:
        cpu: "50m"
        memory: "100Mi"
    image:
      registry: quay.io
      repository: kiwigrid/k8s-sidecar
      tag: 1.22.0
  deploymentStrategy:
    type: Recreate
  persistence:
    type: pvc
    enabled: true
    # storageClassName:
    accessModes:
      - ReadWriteOnce
    size: 5Gi
    # annotations: {}
    finalizers:
      - kubernetes.io/pvc-protection
    # subPath: ""
    # existingClaim:
